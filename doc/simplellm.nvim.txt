simplellm.nvim.txt	Interact with Google LLM API from inside Neovim

==============================================================================
CONTENTS                                                *SimpleLLM*

    1. Features  .......................... |SimpleLLMFeatures|
    2. Installation  ...................... |SimpleLLMInstallation|
    3. Configuration ...................... |SimpleLLMConfiguration|
    4. Commands  .......................... |SimpleLLMCommands|
    5. Functions  ......................... |SimpleLLMFunctions|
    6. Changelog  ......................... |SimpleLLMChangelog|
    7. Credits  ........................... |SimpleLLMCredits|

==============================================================================
1. Features                                        *SimpleLLMFeatures*

SimpleLLM plugin let you ask something to LLM providers without leaving
NeoVim. The plugin is written in Lua and takes its inspiration from
askGemini.nvim. Contrary to other feature-rich plugins available on Github,
this one strives to avoid imposing a specific workflow on the user and to keep
as least opinionated as possible.

It only adds one new command and several Lua functions to your environment
(see also |SimpleLLMCommands| and |SimpleLLMFunctions|). It does not create
any new keymap. You have to configure them by yourself.

The plugin does not rely on third-party plugins. The plugin implementation 
is lazily loaded. There is no need to use the package manager for a lazy 
loading of the plugin.

Other features: ~

• Use Google Gemini and Groq endpoints
• Use any LLM model (including the free ones)
• Setup predefined prompts for later use in the configuration options

Known limits: ~

The plugin has a number of limits, by design.

• The history of the conversation with the LLM is not kept. The AI forgets all
  your previous exchanges. This is because the plugin is designed to be used
  for small edits on a buffer, not for a full conversation. If you want to
  have a full conversation, use your browser and browse to the LLM homepage.
• Calls to the LLM API are made synchronously. This may result in some
  delay when producing the answer, but it feels more natural when updating
  text in the current buffer.

==============================================================================
2. Installation                                    *SimpleLLMInstallation*

The plugin needs a LLM API key to connect to the LLM API. For Google Gemini, you 
have to follow the instructions at https://aistudio.google.com/app/apikey?hl=fr
For Groq, the instructions are available at https://console.groq.com/keys 

Installation with lazy.nvim ~

If you use lazy.nvim as your package manager, you can install SimpleLLM.nvim
with the following specification.
>
 return {
    [...]
    {
	'frodonh/simplellm.nvim',
	opts = {
	    [...]
	}
    }
    [...]
 }

It is unnecessary to call the setup() function if you don't want to customize
the configuration.

Default options ~

The following options are set by default. You don't have to set them in your
rc file:

>
 opts = {
    endpoint = 'gemini',
    gemini = {
      model = 'gemini-2.5-flash'
    },
    groq = {
      model = 'compound-beta'
    }
    language = 'fr',
 }

A number of predefined prompts are also set for French and English languages.

==============================================================================
3. Configuration                                   *SimpleLLMConfiguration*

A few configuration options are provided. They may be defined at startup (with
require 'SimpleLLM'.setup({options}) ). {options} is a table with the
configuration options.

endpoint                 `gemini` or `groq` ; set the default endpoint to use

language                 Code of the language used for the predefined prompts.
                         For the moment only `fr` (French) and `en` (English)
			 are supported. However, the user can add its own
			 language with the `prompts` configuration option and
			 set here the code of the new language to use it.

gemini.api_key           Gemini API key.
                         If not set in the options, the value from the
			 'GEMINI_API_KEY' environment variable is used.

gemini.model             LLM to use through the Gemini endpoint

groq.api_key             Groq API key.
                         If not set in the options, the value from the
			 'GROQ_API_KEY' environment variable is used.

groq.model               LLM to use through the Groq endpoint

model                    LLM model name (default is 'llm-2.0-flash').

prompts                  Predefined prompts. This is a dictionary where the
                         keys are the codes of the languages (`fr`, `en`, or
			 custom language set by the user) and the values are a
			 table with the predefined prompts.

                         Each item in this table should have two keys:
			 • action: (string) Summary name of the action
			 • prompt: (string) Prompt as sent to LLM API

==============================================================================
4. Commands                                        *SimpleLLMCommands*

                                                        *:SimpleLLM*
:[range]SimpleLLM Buffer {prompt}
                         Ask a question to LLM and insert answer
			 before cursor position in the current buffer. If 
			 specified, [prompt] is sent to LLM. If not, the
			 user is asked interactively to choose between a list
			 of predefined prompts.

			 If [range] is given, the prompt is followed by a
			 newline and all the lines in the range.

:[range]SimpleLLM! Buffer {prompt}
                         Ask a question to LLM and insert answer
			 before cursor position in the current buffer. If 
			 specified, [prompt] is sent to LLM. If not, the
			 user is asked interactively to choose between a list
			 of predefined prompts.

			 If [range] is given, the prompt is followed by a
			 newline and all the lines in the range. Unlike
			 the previous plain version, the selection is deleted
			 before the answer is inserted.

			 In other words the selected text is used as the
			 prompt for a LLM request, which may be preceded by
			 another prompt, and is replaced by its answer.

:[range]SimpleLLM Reg {prompt}
                         Ask a question to LLM and fill the unnamed
			 register with the answer. If specified, {prompt} is 
			 sent to LLM. If not, the user is asked interactively
			 to choose between a list of predefined prompts.


			 If [range] is given, the prompt is followed by a
			 newline and all the lines in the range.

:[range]SimpleLLM Reg={name} {prompt}
                         Ask a question to LLM and fill the {name}
			 register with the answer. If specified, {prompt} is 
			 sent to LLM. If not, the user is asked interactively
			 to choose between a list of predefined prompts.

			 {name} should be a single character specifying the
			 buffer which will be filled with the answer of the
			 request to LLM.
			 
			 If [range] is given, the prompt is followed by a
			 newline and all the lines in the range.

:[range]SimpleLLM Scratch {prompt}
                         Ask a question to LLM and display the answer   
                         in a new floating window. If specified, {prompt}
			 is sent to LLM. If not, the user is asked
			 interactively to choose between a list of predefined
			 prompts.

			 The floating window can be closed by pressing <Esc>.
			 The content of the floating window is deleted when 
			 it is closed.

			 If [range] is given, the prompt is followed by a
			 newline and all the lines in the range.

:SimpleLLM set {endpoint} {model}
                         Set a new endpoint and model. The following calls to
			 the API will use those values.

			 For the moment, two endpoints are usable:
			 • `gemini` : Google Gemini API
			 • `groq` : Groq API

			 The lists of available models can be found on the
			 respective API documentations. Some free models can
			 be found by using the auto-completion feature.

==============================================================================
5. Functions                                       *SimpleLLMFunctions*

Functions are in the 'simplellm' module. You have to require the module to
use them. Requiring the module only loads the function declaration ; their
implementation is loaded when they are called.

setup({options})                                     *SimpleLLMSetup*
    Configure options for the plugin.

    Parameters: ~
      • {options}   Dictionary with options. See also
	            |SimpleLLMConfiguration| for configuration parameters.


process({args})                                      *SimpleLLMAsk*
    Process a user command. This is the main function of the plugin. It parses
    the arguments of the command, submits a request to the LLM and populates
    some destination with the answer (either the current buffer, or a
    register, or a new scratch window).

    Parameters: ~
      • {args}      Dictionary of arguments. The table should have the same
	            structure as the {command} argument of the 
		    |nvim_create_user_command()| function (used to define new
		    commands).
		    Since this function matches the call protocol of commands,
		    it is suitable to defe new commands. It is for example
		    used by the |:SimpleLLM| command.
		    The following keys are used:
		    • args: (string) The prompt passed to the command, if any
		    • range: (number) The number of items in the range passed
		      to the function, 0 if no range was selected. If a range
		      was selected, it will be added to the prompt.
		    • line1: (number) The starting line of the command range
		    • line2: (number) The final line of the command range

    Return: ~
        Array of lines sent as answer by the LLM


send_prompt({prompt})                                *SimpleLLMSendPrompt*
    Send a prompt the the LLM API.

    Parameters: ~
      • {prompt}    The text of the prompt which is to be submitted.

    Return: ~
        The text of the answer of the LLM API.

==============================================================================
6. Changelog                                       *SimpleLLMChangelog*

Date: June 24th, 2025
Add Groq endpoint

Date: June 2nd, 2025
Added a feature to use predefined prompts

Date: May 29th, 2025
First version

==============================================================================
7. Credits                                         *SimpleLLMCredits*

This plugin takes its inspiration from askGemini.nvim which is available here:
https://github.com/agusnt/askGemini.nvim/tree/main

 vim:tw=78:ts=8:noet:ft=help:norl:
